@article{Alotibi_2021,
	title        = {Anomaly Detection for Cooperative Adaptive Cruise Control in Autonomous Vehicles Using Statistical Learning and Kinematic Model},
	author       = {Faris Alotibi and Mai Abdelhakim},
	year         = 2021,
	month        = {jun},
	journal      = {IEEE Transactions on Intelligent Transportation Systems},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 22,
	number       = 6,
	pages        = {3468--3478}
}
@article{Chen_2019,
	title        = {Pedestrian Detection for Autonomous Vehicle Using Multi-Spectral Cameras},
	author       = {Zhilu Chen and Xinming Huang},
	year         = 2019,
	journal      = {IEEE Transactions on Intelligent Vehicles},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 4,
	number       = 2,
	pages        = {211--219},
	abstract     = {Pedestrian detection is a critical feature of autonomous vehicle or advanced driver assistance system. This paper presents a novel instrument for pedestrian detection by combining stereo vision cameras with a thermal camera. A new dataset for vehicle applications is built from the test vehicle recorded data when driving on city roads. Data received from multiple cameras are aligned using trifocal tensor with pre-calibrated parameters. Candidates are generated from each image frame using sliding windows across multiple scales. A reconfigurable detector framework is proposed, in which feature extraction and classification are two separate stages. The input to the detector can be the color image, disparity map, thermal data, or any of their combinations. When applying to convolutional channel features, feature extraction utilizes the first three convolutional layers of a pre-trained convolutional neural network cascaded with an AdaBoost classifier. The evaluation results show that it significantly outperforms the traditional histogram of oriented gradients features. The proposed pedestrian detector with multi-spectral cameras can achieve 9\% log-average miss rate. The experimental dataset is made available at http://computing.wpi.edu/dataset.html .},
	keywords     = {Advanced driver assistance systems; Artificial neural networks; autonomous vehicle; Autonomous vehicles; Cameras; Color imagery; Data collection; Datasets; Detectors; Drivers; Feature extraction; Histograms; Image color analysis; Instruments; Machine learning; Multi-spectral camera; pedestrian detection; Sensors; Stereo vision; Tensors; Test vehicles}
}
@inproceedings{Cucchiara,
	title        = {Statistic and knowledge-based moving object detection in traffic scenes},
	author       = {R. Cucchiara and C. Grana and M. Piccardi and A. Prati},
	year         = 2000,
	month        = {Oct},
	booktitle    = {2000 {IEEE} Intelligent Transportation Systems. Proceedings},
	pages        = {27--32},
	abstract     = {The most common approach used for vision-based traffic surveillance consists of a fast segmentation of moving visual objects (MVOs) in the scene together with an intelligent reasoning module capable of identifying, tracking and classifying the MVOs in dependency of the system goal. In this paper we describe our approach for MVOs segmentation in an unstructured traffic environment. We consider complex situations with moving people, vehicles and infrastructures that have different aspect model and motion model. In this case we define a specific approach based on background subtraction with statistic and knowledge-based background update. We show many results of real-time tracking of traffic MVOs in outdoor traffic scene such as roads, parking area intersections, and entrance with barriers. The most common approach used for vision-based traffic surveillance consists of a fast segmentation of moving visual objects (MVOs) in the scene together with an intelligent reasoning module capable of identifying, tracking and classifying the MVOs in dependency of the system goal. In this paper we describe our approach for MVOs segmentation in an unstructured traffic environment. We consider complex situations with moving people, vehicles and infrastructures that have different aspect model and motion model. In this case we define a specific approach based on background subtraction with statistic and knowledge-based background update. We show many results of real-time tracking of traffic MVOs in outdoor traffic scene such as roads, parking area intersections, and entrance with barriers.},
	keywords     = {Statistics; Object detection; Layout; Surveillance; Traffic control; Monitoring; Intelligent transportation systems; Vehicle dynamics; Data mining; Machine vision}
}
@article{Deng_2022,
	title        = {Extended Feature Pyramid Network for Small Object Detection},
	author       = {Chunfang Deng and Mengmeng Wang and Liang Liu and Yong Liu and Yunliang Jiang},
	year         = 2022,
	journal      = {{IEEE} Transactions on Multimedia},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 24,
	pages        = {1968--1979},
	issn         = {1520-9210,1941-0077},
	shortjournal = {IEEE Trans. Multimedia}
}
@article{Gevorgyan-225,
	title        = {SIoU Loss: More Powerful Learning for Bounding Box Regression},
	author       = {Zhora Gevorgyan},
	year         = 2022,
	journal      = {arXiv.org},
	abstract     = {The effectiveness of Object Detection, one of the central problems incomputer vision tasks, highly depends on the definition of the loss function -a measure of how accurately your ML model can predict the expected outcome.Conventional object detection loss functions depend on aggregation of metricsof bounding box regression such as the distance, overlap area and aspect ratioof the predicted and ground truth boxes (i.e. GIoU, CIoU, ICIoU etc). However,none of the methods proposed and used to date considers the direction of themismatch between the desired ground box and the predicted, "experimental" box.This shortage results in slower and less effective convergence as the predictedbox can "wander around" during the training process and eventually end upproducing a worse model. In this paper a new loss function SIoU was suggested,where penalty metrics were redefined considering the angle of the vectorbetween the desired regression. Applied to conventional Neural Networks anddatasets it is shown that SIoU improves both the speed of training and theaccuracy of the inference. The effectiveness of the proposed loss function wasrevealed in a number of simulations and tests.},
	shortjournal = {arXiv.org}
}
@article{Grigorescu_2020,
	title        = {A survey of deep learning techniques for autonomous driving},
	author       = {Sorin Grigorescu and Bogdan Trasnea and Tiberiu Cocias and Gigel Macesanu},
	year         = 2020,
	month        = {apr},
	journal      = {Journal of Field Robotics},
	publisher    = {Wiley},
	volume       = 37,
	number       = 3,
	pages        = {362--386},
	abstract     = {The last decade witnessed increasingly rapid progress in self-driving vehicletechnology, mainly backed up by advances in the area of deep learning andartificial intelligence. The objective of this paper is to survey the currentstate-of-the-art on deep learning technologies used in autonomous driving. Westart by presenting AI-based self-driving architectures, convolutional andrecurrent neural networks, as well as the deep reinforcement learning paradigm.These methodologies form a base for the surveyed driving scene perception, pathplanning, behavior arbitration and motion control algorithms. We investigateboth the modular perception-planning-action pipeline, where each module isbuilt using deep learning methods, as well as End2End systems, which directlymap sensory information to steering commands. Additionally, we tackle currentchallenges encountered in designing AI architectures for autonomous driving,such as their safety, training data sources and computational hardware. Thecomparison presented in this survey helps to gain insight into the strengthsand limitations of deep learning and AI approaches for autonomous driving andassist with design choices}
}
@article{Exdark,
	title        = {Getting to Know Low-light Images with The Exclusively Dark Dataset},
	author       = {Loh, Yuen Peng and Chan, Chee Seng},
	year         = 2019,
	journal      = {Computer Vision and Image Understanding},
	volume       = 178,
	pages        = {30--42}
}
@article{Gupta_2021,
	title        = {Deep learning for object detection and scene perception in self-driving cars: Survey, challenges, and open issues},
	author       = {Abhishek Gupta and Alagan Anpalagan and Ling Guan and Ahmed Shaharyar Khwaja},
	year         = 2021,
	month        = {jul},
	journal      = {Array},
	booktitle    = {2011 IEEE intelligent vehicles symposium (IV)},
	publisher    = {Elsevier {BV}},
	volume       = 10,
	pages        = {10--57}
}
@article{Hong2016,
	title        = {PVANet: Lightweight Deep Neural Networks for Real-time Object Detection},
	author       = {Sanghoon Hong and Byungseok Roh and Kye-Hyeon Kim and Yeongjae Cheon and Minje Park},
	year         = 2016,
	journal      = {arXiv.org},
	shortjournal = {arXiv.org}
}
@article{Hsu_2021,
	title        = {Adaptive Fusion of Multi-Scale {YOLO} for Pedestrian Detection},
	author       = {Wei-Yen Hsu and Wen-Yen Lin},
	year         = 2021,
	journal      = {{IEEE} Access},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 9,
	pages        = {110063--110073}
}
@article{Hu_2021,
	title        = {{PAG}-{YOLO}: A Portable Attention-Guided {YOLO} Network for Small Ship Detection},
	author       = {Jianming Hu and Xiyang Zhi and Tianjun Shi and Wei Zhang and Yang Cui and Shenggang Zhao},
	year         = 2021,
	month        = {aug},
	journal      = {Remote Sensing},
	publisher    = {{MDPI} {AG}},
	volume       = 13,
	number       = 16,
	pages        = 3059,
	issn         = {2072-4292},
	abstract     = {The YOLO network has been extensively employed in the field of ship detection in optical images. However, the YOLO model rarely considers the global and local relationships in the input image, which limits the final target prediction performance to a certain extent, especially for small ship targets. To address this problem, we propose a novel small ship detection method, which improves the detection accuracy compared with the YOLO-based network architecture and does not increase the amount of computation significantly. Specifically, attention mechanisms in spatial and channel dimensions are proposed to adaptively assign the importance of features in different scales. Moreover, in order to improve the training efficiency and detection accuracy, a new loss function is employed to constrain the detection step, which enables the detector to learn the shape of the ship target more efficiently. The experimental results on a public and high-quality ship dataset indicate that our method realizes state-of-the-art performance in comparison with several widely used advanced approaches.},
	language     = {en},
	shortjournal = {Remote Sensing}
}
@article{Jiang_2022_08,
	title        = {Object detection from {UAV} thermal infrared images and videos using {YOLO} models},
	author       = {Chenchen Jiang and Huazhong Ren and Xin Ye and Jinshun Zhu and Hui Zeng and Yang Nan and Min Sun and Xiang Ren and Hongtao Huo},
	year         = 2022,
	month        = {aug},
	journal      = {International Journal of Applied Earth Observation and Geoinformation},
	publisher    = {Elsevier {BV}},
	volume       = 112,
	pages        = {102912},
	issn         = {1569-8432}
}
@article{Jiang_2022_10,
	title        = {An Attention Mechanism-Improved {YOLOv}7 Object Detection Algorithm for Hemp Duck Count Estimation},
	author       = {Kailin Jiang and Tianyu Xie and Rui Yan and Xi Wen and Danyang Li and Hongbo Jiang and Ning Jiang and Ling Feng and Xuliang Duan and Jianjun Wang},
	year         = 2022,
	month        = {oct},
	journal      = {Agriculture},
	publisher    = {{MDPI} {AG}},
	volume       = 12,
	number       = 10,
	pages        = 1659,
	abstract     = {Stocking density presents a key factor affecting livestock and poultry       production on a large scale as well as animal welfare. However, the       current manual counting method used in the hemp duck breeding industry is       inefficient, costly in labor, less accurate, and prone to double counting       and omission. In this regard, this paper uses deep learning algorithms to       achieve real-time monitoring of the number of dense hemp duck flocks and       to promote the development of the intelligent farming industry. We       constructed a new large-scale hemp duck object detection image dataset,       which contains 1500 hemp duck object detection full-body frame labeling       and head-only frame labeling. In addition, this paper proposes an improved       attention mechanism YOLOv7 algorithm, CBAM-YOLOv7, adding three CBAM       modules to the backbone network of YOLOv7 to improve the network's ability       to extract features and introducing SE-YOLOv7 and ECA-YOLOv7 for       comparison experiments. The experimental results show that CBAM-YOLOv7 had       higher precision, and the recall, mAP\@0.5, and mAP\@0.5:0.95 were slightly       improved. The evaluation index value of CBAM-YOLOv7 improved more than       those of SE-YOLOv7 and ECA-YOLOv7. In addition, we also conducted a       comparison test between the two labeling methods and found that the       head-only labeling method led to the loss of a high volume of feature       information, and the full-body frame labeling method demonstrated a better       detection effect. The results of the algorithm performance evaluation show       that the intelligent hemp duck counting method proposed in this paper is       feasible and can promote the development of smart reliable automated duck       counting.}
}
@misc{JocherChaurasia-220,
	title        = {ultralytics/yolov5: v7.0 - YOLOv5 SOTA Realtime Instance Segmentation},
	author       = {Glenn Jocher and Ayush Chaurasia and Alex Stoken and Jirka Borovec and NanoCode012 and Yonghye Kwon and Kalen Michael and TaoXie and Jiacong Fang and imyhxy and Lorna and Zeng Yifu and Colin Wong and Abhiram V and Diego Montes and Zhiqiang Wang and Cristi Fati and Jebastin Nadar and Laughing and UnglvKitDe and Victor Sonck and tkianai and yxNONG and Piotr Skalski and Adam Hogan and Dhruv Nair and Max Strobel and Mrinal Jain},
	year         = 2022,
	month        = nov,
	publisher    = {Zenodo},
	version      = {v7.0}
}
@article{Kiran_2022,
	title        = {Deep Reinforcement Learning for Autonomous Driving: A Survey},
	author       = {B Ravi Kiran and Ibrahim Sobh and Victor Talpaert and Patrick Mannion and Ahmad A. Al Sallab and Senthil Yogamani and Patrick Perez},
	year         = 2022,
	month        = {jun},
	journal      = {IEEE Transactions on Intelligent Transportation Systems},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 23,
	number       = 6,
	pages        = {4909--4926},
	abstract     = {With the development of deep representation learning, the domain of reinforcement learning (RL) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical RL algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in RL are discussed. With the development of deep representation learning, the domain of reinforcement learning (RL) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical RL algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in RL are discussed.},
	keywords     = {Reinforcement learning; Autonomous vehicles; Task analysis; Planning; Robot sensing systems; Pipelines; Decision making; Deep reinforcement learning; autonomous driving; imitation learning; inverse reinforcement learning; controller learning; trajectory optimisation; motion planning; safe reinforcement learning}
}
@article{Law_2019,
	title        = {CornerNet: Detecting Objects as Paired Keypoints},
	author       = {Hei Law and Jia Deng},
	year         = 2019,
	month        = {aug},
	journal      = {International Journal of Computer Vision},
	publisher    = {Springer Science and Business Media {LLC}},
	volume       = 128,
	number       = 3,
	pages        = {642--656},
	abstract     = {We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2\% AP on MS COCO, outperforming all existing one-stage detectors.},
	shortjournal = {arXiv.org}
}
@article{Li_2023,
	title        = {{YOLOSR}-{IST}: A deep learning method for small target detection in infrared remote sensing images based on super-resolution and {YOLO}},
	author       = {Ronghao Li and Ying Shen},
	year         = 2023,
	month        = {jul},
	journal      = {Signal Processing},
	publisher    = {Elsevier {BV}},
	volume       = 208,
	pages        = 108962,
	issn         = {0165-1684}
}
@incollection{Liu_2016,
	title        = {SSD: Single Shot MultiBox Detector},
	author       = {Wei Liu and Dragomir Anguelov and Dumitru Erhan and Christian Szegedy and Scott Reed and Cheng-Yang Fu and Alexander C. Berg},
	year         = 2016,
	booktitle    = {Computer Vision {\textendash} {ECCV} 2016},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {21--37}
}
@article{Liu_2019,
	title        = {Deep Learning for Generic Object Detection: A Survey},
	author       = {Li Liu and Wanli Ouyang and Xiaogang Wang and Paul Fieguth and Jie Chen and Xinwang Liu and Matti Pietikainen},
	year         = 2019,
	month        = {oct},
	journal      = {International Journal of Computer Vision},
	publisher    = {Springer Science and Business Media {LLC}},
	volume       = 128,
	number       = 2,
	pages        = {261--318}
}
@article{Liu_2021,
	title        = {Computing Systems for Autonomous Driving: State of the Art and Challenges},
	author       = {Liangkai Liu and Sidi Lu and Ren Zhong and Baofu Wu and Yongtao Yao and Qingyang Zhang and Weisong Shi},
	year         = 2021,
	month        = {apr},
	journal      = {IEEE Internet of Things Journal},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 8,
	number       = 8,
	pages        = {6469--6486},
	abstract     = {The recent proliferation of computing technologies (e.g., sensors, computer vision, machine learning, and hardware acceleration) and the broad deployment of communication mechanisms (e.g., dedicated short-range communication, cellular vehicle-to-everything, 5G) have pushed the horizon of autonomous driving, which automates the decision and control of vehicles by leveraging the perception results based on multiple sensors. The key to the success of these autonomous systems is making a reliable decision in real-time fashion. However, accidents and fatalities caused by early deployed autonomous vehicles arise from time to time. The real traffic environment is too complicated for current autonomous driving computing systems to understand and handle. In this article, we present state-of-the-art computing systems for autonomous driving, including seven performance metrics and nine key technologies, followed by 12 challenges to realize autonomous driving. We hope this article will gain attention from both the computing and automotive communities and inspire more research in this direction. The recent proliferation of computing technologies (e.g., sensors, computer vision, machine learning, and hardware acceleration) and the broad deployment of communication mechanisms (e.g., dedicated short-range communication, cellular vehicle-to-everything, 5G) have pushed the horizon of autonomous driving, which automates the decision and control of vehicles by leveraging the perception results based on multiple sensors. The key to the success of these autonomous systems is making a reliable decision in real-time fashion. However, accidents and fatalities caused by early deployed autonomous vehicles arise from time to time. The real traffic environment is too complicated for current autonomous driving computing systems to understand and handle. In this article, we present state-of-the-art computing systems for autonomous driving, including seven performance metrics and nine key technologies, followed by 12 challenges to realize autonomous driving. We hope this article will gain attention from both the computing and automotive communities and inspire more research in this direction.},
	keywords     = {Autonomous vehicles; Sensors; Sensor systems; Security; Cameras; Real-time systems; Radar; Autonomous driving; challenges; computing systems}
}
@article{Liu_2021_06,
	title        = {A survey and performance evaluation of deep learning methods for small object detection},
	author       = {Yang Liu and Peng Sun and Nickolas Wergeles and Yi Shang},
	year         = 2021,
	month        = {jun},
	journal      = {Expert Systems with Applications},
	publisher    = {Elsevier {BV}},
	volume       = 172,
	pages        = 114602,
	issn         = {0957-4174}
}
@article{Liu_2022,
	title        = {{CEAM}-{YOLOv}7: Improved {YOLOv}7 Based on Channel Expansion and Attention Mechanism for Driver Distraction Behavior Detection},
	author       = {Shugang Liu and Yujie Wang and Qiangguo Yu and Hongli Liu and Zhan Peng},
	year         = 2022,
	journal      = {IEEE Access},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 10,
	pages        = {129116--129124}
}
@inproceedings{Long_2015,
	title        = {Fully Convolutional Networks for Semantic Segmentation},
	author       = {Jonathan Long and Evan Shelhamer and Trevor Darrell},
	year         = 2015,
	month        = {jun},
	journal      = {arXiv.org},
	booktitle    = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages        = {3431--3440}
}
@inproceedings{Najibi_2017,
	title        = {{SSH}: Single Stage Headless Face Detector},
	author       = {Mahyar Najibi and Pouya Samangouei and Rama Chellappa and Larry S. Davis},
	year         = 2017,
	month        = {oct},
	booktitle    = {2017 {IEEE} International Conference on Computer Vision},
	location     = {Venice},
	pages        = {4885--4894}
}
@inproceedings{Najibi_2019,
	title        = {AutoFocus: Efficient Multi-Scale Inference},
	author       = {Mahyar Najibi and Bharat Singh and Larry Davis},
	year         = 2019,
	month        = {oct},
	booktitle    = {2019 IEEE/CVF International Conference on Computer Vision},
	pages        = {9745--9755}
}
@inproceedings{Newell_2016,
	title        = {Stacked Hourglass Networks for Human Pose Estimation},
	author       = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
	year         = 2016,
	booktitle    = {Lecture Notes in Computer Science},
	address      = {Cham},
	volume       = 9912,
	pages        = {483--499}
}
@article{Outay_2020,
	title        = {Applications of unmanned aerial vehicle ({UAV}) in road safety, traffic and highway infrastructure management: Recent advances and challenges},
	author       = {Fatma Outay and Hanan Abdullah Mengash and Muhammad Adnan},
	year         = 2020,
	month        = {nov},
	journal      = {Transportation Research Part A: Policy and Practice},
	publisher    = {Elsevier {BV}},
	volume       = 141,
	pages        = {116--129},
	abstract     = {For next-generation smart cities, small UAVs (also known as drones) are vital to incorporate in airspace for advancing the transportation systems. This paper presents a review of recent developments in relation to the application of UAVs in three major domains of transportation, namely; road safety, traffic monitoring and highway infrastructure management. Advances in computer vision algorithms to extract key features from UAV acquired videos and images are discussed along with the discussion on improvements made in traffic flow analysis methods, risk assessment and assistance in accident investigation and damage assessments for bridges and pavements. Additionally, barriers associated with the wide-scale deployment of UAVs technology are identified and countermeasures to overcome these barriers are discussed, along with their implications.},
	keywords     = {Unmanned aerial vehicles (UAVs);Road safety;Traffic monitoring;Highway infrastructure management;Applications}
}
@article{Padilla_Carrasco_2023,
	title        = {T-{YOLO}: Tiny Vehicle Detection Based on {YOLO} and Multi-Scale Convolutional Neural Networks},
	author       = {Daniel Padilla Carrasco and Hatem A. Rashwan and Miguel {'{A}}ngel Garc{'{\i}}a and Dom{\`{e}}nec Puig},
	year         = 2023,
	journal      = {IEEE Access},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 11,
	pages        = {22430--22440},
	abstract     = {To solve real-life problems for different smart city applications, using deep Neural Network, such as parking occupancy detection, requires fine-tuning of these networks. For large parking, it is desirable to use a cenital-plane camera located at a high distance that allows the monitoring of the entire parking space or a large parking area with only one camera. Today's most popular object detection models, such as YOLO, achieve good precision scores at real-time speed. However, if we use our own data different from that of the general-purpose datasets, such as COCO and ImageNet, we have a large margin for improvisation. In this paper, we propose a modified, yet lightweight, deep object detection model based on the YOLO-v5 architecture. The proposed model can detect large, small, and tiny objects. Specifically, we propose the use of a multi-scale mechanism to learn deep discriminative feature representations at different scales and automatically determine the most suitable scales for detecting objects in a scene (i.e., in our case vehicles). The proposed multi-scale module reduces the number of trainable parameters compared to the original YOLO-v5 architecture. The experimental results also demonstrate that precision is improved by a large margin. In fact, as shown in the experiments, the results show a small reduction from 7.28 million parameters of the YOLO-v5-S profile to 7.26 million parameters in our model. In addition, we reduced the detection speed by inferring 30 fps compared to the YOLO-v5-L/X profiles. In addition, the tiny vehicle detection performance was significantly improved by 33\% compared to the YOLO-v5-X profile. To solve real-life problems for different smart city applications, using deep Neural Network, such as parking occupancy detection, requires fine-tuning of these networks. For large parking, it is desirable to use a cenital-plane camera located at a high distance that allows the monitoring of the entire parking space or a large parking area with only one camera. Today's most popular object detection models, such as YOLO, achieve good precision scores at real-time speed. However, if we use our own data different from that of the general-purpose datasets, such as COCO and ImageNet, we have a large margin for improvisation. In this paper, we propose a modified, yet lightweight, deep object detection model based on the YOLO-v5 architecture. The proposed model can detect large, small, and tiny objects. Specifically, we propose the use of a multi-scale mechanism to learn deep discriminative feature representations at different scales and automatically determine the most suitable scales for detecting objects in a scene (i.e., in our case vehicles). The proposed multi-scale module reduces the number of trainable parameters compared to the original YOLO-v5 architecture. The experimental results also demonstrate that precision is improved by a large margin. In fact, as shown in the experiments, the results show a small reduction from 7.28 million parameters of the YOLO-v5-S profile to 7.26 million parameters in our model. In addition, we reduced the detection speed by inferring 30 fps compared to the YOLO-v5-L/X profiles. In addition, the tiny vehicle detection performance was significantly improved by 33\% compared to the YOLO-v5-X profile.},
	keywords     = {Object detection; Cameras; Feature extraction; Computational modeling; Automobiles; Convolutional neural networks; Detectors; Convolutional neural networks; tiny objects; smart parking}
}
@article{Paneru_2021,
	title        = {Computer vision applications in construction: Current state, opportunities \& challenges},
	author       = {Suman Paneru and Idris Jeelani},
	year         = 2021,
	month        = {dec},
	journal      = {Automation in Construction},
	publisher    = {Elsevier {BV}},
	volume       = 132,
	pages        = 103940
}
@article{Petit_2014,
	title        = {Potential Cyberattacks on Automated Vehicles},
	author       = {Jonathan Petit and Steven E. Shladover},
	year         = 2014,
	journal      = {{IEEE} Transactions on Intelligent Transportation Systems},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 16,
	number       = 2,
	pages        = {1--11}
}
@article{Rabbi_2020,
	title        = {Small-Object Detection in Remote Sensing Images with End-to-End Edge-Enhanced {GAN} and Object Detector Network},
	author       = {Jakaria Rabbi and Nilanjan Ray and Matthias Schubert and Subir Chowdhury and Dennis Chao},
	year         = 2020,
	month        = {may},
	journal      = {Remote Sensing},
	publisher    = {{MDPI} {AG}},
	volume       = 12,
	number       = 9,
	pages        = 1432,
	issn         = {2072-4292},
	abstract     = {The detection performance of small objects in remote sensing images has not been satisfactory compared to large objects, especially in low-resolution and noisy images. A generative adversarial network (GAN)-based model called enhanced super-resolution GAN (ESRGAN) showed remarkable image enhancement performance, but reconstructed images usually miss high-frequency edge information. Therefore, object detection performance showed degradation for small objects on recovered noisy and low-resolution remote sensing images. Inspired by the success of edge enhanced GAN (EEGAN) and ESRGAN, we applied a new edge-enhanced super-resolution GAN (EESRGAN) to improve the quality of remote sensing images and used different detector networks in an end-to-end manner where detector loss was backpropagated into the EESRGAN to improve the detection performance. We proposed an architecture with three components: ESRGAN, EEN, and Detection network. We used residual-in-residual dense blocks (RRDB) for both the ESRGAN and EEN, and for the detector network, we used a faster region-based convolutional network (FRCNN) (two-stage detector) and a single-shot multibox detector (SSD) (one stage detector). Extensive experiments on a public (car overhead with context) dataset and another self-assembled (oil and gas storage tank) satellite dataset showed superior performance of our method compared to the standalone state-of-the-art object detectors.},
	language     = {en},
	shortjournal = {Remote Sensing}
}
@inproceedings{Redmon_2016,
	title        = {You Only Look Once: Unified, Real-Time Object Detection},
	author       = {Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
	year         = 2016,
	month        = {jun},
	journal      = {arXiv.org},
	booktitle    = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages        = {779--788},
	abstract     = {We present YOLO, a new approach to object detection. Prior work on objectdetection repurposes classifiers to perform detection. Instead, we frame objectdetection as a regression problem to spatially separated bounding boxes andassociated class probabilities. A single neural network predicts bounding boxesand class probabilities directly from full images in one evaluation. Since thewhole detection pipeline is a single network, it can be optimized end-to-enddirectly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processesimages in real-time at 45 frames per second. A smaller version of the network,Fast YOLO, processes an astounding 155 frames per second while still achievingdouble the mAP of other real-time detectors. Compared to state-of-the-artdetection systems, YOLO makes more localization errors but is far less likelyto predict false detections where nothing exists. Finally, YOLO learns verygeneral representations of objects. It outperforms all other detection methods,including DPM and R-CNN, by a wide margin when generalizing from natural imagesto artwork on both the Picasso Dataset and the People-Art Dataset.},
	keywords     = {Computer Science - Computer Vision and Pattern Recognition}
}
@article{Shen_2023,
	title        = {Multiple information perception-based attention in {YOLO} for underwater object detection},
	author       = {Xin Shen and Huibing Wang and Tianxiang Cui and Zhicheng Guo and Xianping Fu},
	year         = 2023,
	month        = {may},
	journal      = {The Visual Computer},
	publisher    = {Springer Science and Business Media {LLC}},
	abstract     = {Underwater object detection is a prerequisite for underwater robots to achieve autonomous operation and ocean exploration. However, poor imaging quality, harsh underwater environments, and concealed underwater targets greatly aggravate the difficulty of underwater object detection. In order to reduce underwater background interference and improve underwater object perception, we propose a multiple information perception-based attention module (MIPAM), which is mainly composed of five processes. In information preprocessing, spatial downsampling and channel splitting control parameters and computations of attention module by reducing dimension sizes. In information collection, channel-level information collection and spatial-level information collection enhance the semantic information expression by perceiving multi-dimensional dependency information, multi-dimensional structure information and multi-dimensional global information. In information interaction, channel-driven information interaction and spatial-driven information interaction stimulate the intrinsic interaction potential by further perceiving multi-dimensional diversity information. Adaptive feature fusion further improves the information interaction quality by allocating learnable parameters. In attention activation, the multi-branch structure enhances the attention calibration efficiency by generating multiple attention. In information postprocessing, channel concatenation and spatial upsampling realize the plug-and-play of attention module by restoring original feature states. In order to meet the high-precision and real-time requirements for underwater object detection, we integrate MIPAM into YOLO detectors. The experimental results indicate that our work brings significant performance gains for underwater detection tasks. Our work also provides some performance improvements for other detection tasks, which shows the ideal generalization ability.}
}
@article{Shi_2019,
	title        = {A feature learning approach based on {XGBoost} for driving assessment and risk prediction},
	author       = {Xiupeng Shi and Yiik Diew Wong and Michael Zhi-Feng Li and Chandrasekar Palanisamy and Chen Chai},
	year         = 2019,
	month        = {aug},
	journal      = {Accident Analysis \& Prevention},
	publisher    = {Elsevier {BV}},
	volume       = 129,
	pages        = {170--179},
	abstract     = {This study designs a framework of feature extraction and selection, to assess vehicle driving and predict risk levels. The framework integrates learning-based feature selection, unsupervised risk rating, and imbalanced data resampling. For each vehicle, about 1300 driving behaviour features are extracted from trajectory data, which produce in-depth and multi-view measures on behaviours. To estimate the risk potentials of vehicles in driving, unsupervised data labelling is proposed. Based on extracted risk indicator features, vehicles are clustered into various groups labelled with graded risk levels. Data under-sampling of the safe group is performed to reduce the risk-safe class imbalance. Afterwards, the linkages between behaviour features and corresponding risk levels are built using XGBoost, and key features are identified according to feature importance ranking and recursive elimination. The risk levels of vehicles in driving are predicted based on key features selected. As a case study, NGSIM trajectory data are used in which four risk levels are clustered by Fuzzy C-means, 64 key behaviour features are identified, and an overall accuracy of 89\% is achieved for behaviour-based risk prediction. Findings show that this approach is effective and reliable to identify important features for driving assessment, and achieve an accurate prediction of risk levels.},
	keywords     = {Driving behaviour;Feature learning;XGBoost;Risk prediction}
}
@article{Song_2019,
	title        = {Vision-based vehicle detection and counting system using deep learning in highway scenes},
	author       = {Huansheng Song and Haoxiang Liang and Huaiyu Li and Zhe Dai and Xu Yun},
	year         = 2019,
	journal      = {European Transport Research Review},
	publisher    = {Springer Science and Business Media {LLC}},
	volume       = 11,
	number       = 1,
	pages        = {1--16},
	abstract     = {Intelligent vehicle detection and counting are becoming increasingly important in the field of highway management. However, due to the different sizes of vehicles, their detection remains a challenge that directly affects the accuracy of vehicle counts. To address this issue, this paper proposes a vision-based vehicle detection and counting system. A new high definition highway vehicle dataset with a total of 57,290 annotated instances in 11,129 images is published in this study. Compared with the existing public datasets, the proposed dataset contains annotated tiny objects in the image, which provides the complete data foundation for vehicle detection based on deep learning. In the proposed vehicle detection and counting system, the highway road surface in the image is first extracted and divided into a remote area and a proximal area by a newly proposed segmentation method; the method is crucial for improving vehicle detection. Then, the above two areas are placed into the YOLOv3 network to detect the type and location of the vehicle. Finally, the vehicle trajectories are obtained by the ORB algorithm, which can be used to judge the driving direction of the vehicle and obtain the number of different vehicles. Several highway surveillance videos based on different scenes are used to verify the proposed methods. The experimental results verify that using the proposed segmentation method can provide higher detection accuracy, especially for the detection of small vehicle objects. Moreover, the novel strategy described in this article performs notably well in judging driving direction and counting vehicles. This paper has general practical significance for the management and control of highway scenes.;Abstract Intelligent vehicle detection and counting are becoming increasingly important in the field of highway management. However, due to the different sizes of vehicles, their detection remains a challenge that directly affects the accuracy of vehicle counts. To address this issue, this paper proposes a vision-based vehicle detection and counting system. A new high definition highway vehicle dataset with a total of 57,290 annotated instances in 11,129 images is published in this study. Compared with the existing public datasets, the proposed dataset contains annotated tiny objects in the image, which provides the complete data foundation for vehicle detection based on deep learning. In the proposed vehicle detection and counting system, the highway road surface in the image is first extracted and divided into a remote area and a proximal area by a newly proposed segmentation method; the method is crucial for improving vehicle detection. Then, the above two areas are placed into the YOLOv3 network to detect the type and location of the vehicle. Finally, the vehicle trajectories are obtained by the ORB algorithm, which can be used to judge the driving direction of the vehicle and obtain the number of different vehicles. Several highway surveillance videos based on different scenes are used to verify the proposed methods. The experimental results verify that using the proposed segmentation method can provide higher detection accuracy, especially for the detection of small vehicle objects. Moreover, the novel strategy described in this article performs notably well in judging driving direction and counting vehicles. This paper has general practical significance for the management and control of highway scenes.;},
	keywords     = {Algorithms; Automotive Engineering; Civil Engineering; Counting; Datasets; Deep learning; Engineering; High definition; Highway management; Image detection; Image segmentation; Intelligent vehicles; Machine learning; Object recognition; Original Paper; Regional/Spatial Science; Traffic surveillance; Transportation; Vehicle counting; Vehicle dataset; Vehicle detection; Vehicles; Vision}
}
@article{Srivastava_2021,
	title        = {A survey of deep learning techniques for vehicle detection from {UAV} images},
	author       = {Srishti Srivastava and Sarthak Narayan and Sparsh Mittal},
	year         = 2021,
	month        = {aug},
	journal      = {Journal of Systems Architecture},
	publisher    = {Elsevier {BV}},
	volume       = 117,
	pages        = {102--152},
	abstract     = {"Unmanned aerial vehicles" (UAVs) are now being used for a wide range of surveillance applications. Specifically, the detection of on-ground vehicles from UAV images has attracted significant attention due to its potential in applications such as traffic management, parking lot management, and facilitating rescue operations in disaster zones and rugged terrains. This paper presents a survey of deep learning techniques for performing on-ground vehicle detection from aerial imagery captured using UAVs (also known as drones). We review the works in terms of their approach to improve accuracy and reduce computation overhead and their optimization objective. We show the similarities and differences of various techniques and also highlight the future challenges in this area. This survey will benefit researchers in the area of artificial intelligence, traffic surveillance, and applications of UAVs.;Keywords Review; Drone; "Unmanned aerial vehicle" (UAV); Deep learning; Vehicle detection; Object detection "Unmanned aerial vehicles" (UAVs) are now being used for a wide range of surveillance applications. Specifically, the detection of on-ground vehicles from UAV images has attracted significant attention due to its potential in applications such as traffic management, parking lot management, and facilitating rescue operations in disaster zones and rugged terrains. This paper presents a survey of deep learning techniques for performing on-ground vehicle detection from aerial imagery captured using UAVs (also known as drones). We review the works in terms of their approach to improve accuracy and reduce computation overhead and their optimization objective. We show the similarities and differences of various techniques and also highlight the future challenges in this area. This survey will benefit researchers in the area of artificial intelligence, traffic surveillance, and applications of UAVs. Author Affiliation: (a) CSE Department, IIT Dharwad, India (b) ECE Department, NIT Trichy, India (c) ECE Department, IIT Roorkee, India       extasteriskcentered Corresponding author. Article History: Received 3 February 2021; Revised 21 April 2021; Accepted 24 April 2021 (footnote)1 This work was supported in part by Semiconductor Research Corporation (SRC), USA , grant number 2020-IR-2972. Byline: Srishti Srivastava [srishtisrivastava.ai\@gmail.com] (a), Sarthak Narayan [sarthak.narayan.nitt\@gmail.com] (b), Sparsh Mittal [sparshfec\@iitr.ac.in] (c, extasteriskcentered,1);},
	keywords     = {Artificial intelligence; Deep learning; Drone; Drone aircraft; Methods; Object detection; Review; Surveillance equipment; Surveys; Vehicle detection; Unmanned aerial vehicle (UAV)}
}
@article{Wang_2008,
	title        = {Automatic Vehicle Detection Using Local Features: A Statistical Approach},
	author       = {Chi-Chen Raxle Wang and Jenn-Jier James Lien},
	year         = 2008,
	month        = {mar},
	journal      = {IEEE Transactions on Intelligent Transportation Systems},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 9,
	number       = 1,
	pages        = {83--96},
	abstract     = {This paper develops a novel statistical approach for automatic vehicle detection based on local features that are located within three significant subregions of the image. In the detection process, each subregion is projected onto its associated eigenspace and independent basis space to generate a principal components analysis (PCA) weight vector and an independent component analysis (ICA) coefficient vector, respectively. A likelihood evaluation process is then performed based on the estimated joint probability of the projection weight vectors and the coefficient vectors of the subregions with position information. The use of subregion position information minimizes the risk of false acceptances, whereas the use of PCA to model the low-frequency components of the eigenspace and ICA to model the high-frequency components of the residual space improves the tolerance of the detection process toward variations in the illumination conditions and vehicle pose. The use of local features not only renders the system more robust toward partial occlusions but also reduces the computational overhead. The computational costs are further reduced by eliminating the requirement for an ICA residual image reconstruction process and by computing the likelihood probability using a weighted Gaussian mixture model, whose parameters and weights are iteratively estimated using an expectation-maximization algorithm. This paper develops a novel statistical approach for automatic vehicle detection based on local features that are located within three significant subregions of the image. In the detection process, each subregion is projected onto its associated eigenspace and independent basis space to generate a principal components analysis (PCA) weight vector and an independent component analysis (ICA) coefficient vector, respectively. A likelihood evaluation process is then performed based on the estimated joint probability of the projection weight vectors and the coefficient vectors of the subregions with position information. The use of subregion position information minimizes the risk of false acceptances, whereas the use of PCA to model the low-frequency components of the eigenspace and ICA to model the high-frequency components of the residual space improves the tolerance of the detection process toward variations in the illumination conditions and vehicle pose. The use of local features not only renders the system more robust toward partial occlusions but also reduces the computational overhead. The computational costs are further reduced by eliminating the requirement for an ICA residual image reconstruction process and by computing the likelihood probability using a weighted Gaussian mixture model, whose parameters and weights are iteratively estimated using an expectation-maximization algorithm.},
	keywords     = {Vehicle detection; Computer vision; Independent component analysis; Principal component analysis; Performance evaluation; Lighting; Space vehicles; Rendering (computer graphics); Robustness; Computational efficiency; Automatic vehicle detection; expectation-maximization (EM); independent component analysis (ICA); local feature; principal components analysis (PCA); weighted Gaussian mixture model (GMM)}
}
@article{Wang_2019,
	title        = {Feature extraction and dynamic identification of drivers' emotions},
	author       = {Xiaoyuan Wang and Yaqi Liu and Fang Wang and Jianqiang Wang and Liping Liu and Jingheng Wang},
	year         = 2019,
	month        = {apr},
	journal      = {Transportation Research Part F: Traffic Psychology and Behaviour},
	publisher    = {Elsevier {BV}},
	volume       = 62,
	pages        = {175--191},
	abstract     = {Human emotions are the revulsive of intentions. It's an important premise to identify drivers' emotions dynamically and correctly for the realization of drivers' intentions identification, active vehicle security warning and mind control driving. It is also an essential requirement for the microscopic research of traffic flow theory. Taking the car-following condition as an example, multi-source and dynamic data of human-vehicle-environment under drivers' different emotional states was obtained through emotions induced experiments, actual driving experiments and virtual driving experiments in this paper. The main influencing factors of typical driving emotions were extracted with factor analysis method and emotion identification model was established based on the fuzzy comprehensive evaluation and PAD emotional model. The emotions of joy, anger, sadness and fear can be identified online. The rationality and validity of the emotions feature extraction and identification model were verified through the experiments of actual driving, virtual driving and interactive simulation. The theoretical foundation for the study of emotion guidance mechanism of drivers' intentions can be provided.;Keywords Car-following; Drivers; Behavior; Emotion; Feature extraction; Identification Highlightsextasteriskcentered The idea of intention identification under the guidance of emotion was proposed.     extasteriskcentered Diversified emotion induction methods were used to get driving emotions.    extasteriskcentered The dynamic and on-line recognition of typical driver's emotions was realized.      extasteriskcentered Diversified driving experiments were used to verify the model. Human emotions are the revulsive of intentions. It's an important premise to identify drivers' emotions dynamically and correctly for the realization of drivers' intentions identification, active vehicle security warning and mind control driving. It is also an essential requirement for the microscopic research of traffic flow theory. Taking the car-following condition as an example, multi-source and dynamic data of human-vehicle-environment under drivers' different emotional states was obtained through emotions induced experiments, actual driving experiments and virtual driving experiments in this paper. The main influencing factors of typical driving emotions were extracted with factor analysis method and emotion identification model was established based on the fuzzy comprehensive evaluation and PAD emotional model. The emotions of joy, anger, sadness and fear can be identified online. The rationality and validity of the emotions feature extraction and identification model were verified through the experiments of actual driving, virtual driving and interactive simulation. The theoretical foundation for the study of emotion guidance mechanism of drivers' intentions can be provided. Author Affiliation: (a) College of Electromechanical Engineering, Qingdao University of Science and Technology, Qingdao 266000, China (b) School of Transportation and Vehicle Engineering, Shandong University of Technology, Zibo 255000, China (c) Joint Laboratory for Internet of Vehicles, Ministry of Education -- China Mobile Communications Corporation, Tsinghua University, Beijing 100084, China (d) Shandong Zibo Experimental High School, Zibo 255000, China       extasteriskcentered Corresponding author at: College of Electromechanical Engineering, Qingdao University of Science and Technology, Qingdao 266000, China. Article History: Received 14 November 2016; Revised 2 December 2017; Accepted 6 January 2019 Byline: Xiaoyuan Wang [wangxiaoyuan\@sdut.edu.cn] (a,c,        extasteriskcentered), Yaqi Liu (b), Fang Wang (b), Jianqiang Wang (c), Liping Liu (b), Jingheng Wang (d);       extbullet{}The idea of intention identification under the guidance of emotion was proposed.     extbullet{}Diversified emotion induction methods were used to get driving emotions.     extbullet{}The dynamic and on-line recognition of typical driver's emotions was realized.      extbullet{}Diversified driving experiments were used to verify the model. Human emotions are the revulsive of intentions. It's an important premise to identify drivers' emotions dynamically and correctly for the realization of drivers' intentions identification, active vehicle security warning and mind control driving. It is also an essential requirement for the microscopic research of traffic flow theory. Taking the car-following condition as an example, multi-source and dynamic data of human-vehicle-environment under drivers' different emotional states was obtained through emotions induced experiments, actual driving experiments and virtual driving experiments in this paper. The main influencing factors of typical driving emotions were extracted with factor analysis method and emotion identification model was established based on the fuzzy comprehensive evaluation and PAD emotional model. The emotions of joy, anger, sadness and fear can be identified online. The rationality and validity of the emotions feature extraction and identification model were verified through the experiments of actual driving, virtual driving and interactive simulation. The theoretical foundation for the study of emotion guidance mechanism of drivers' intentions can be provided.;},
	keywords     = {Automobile drivers; Behavior; Car following; Computer simulation; Drivers; Driving; Emotion; Emotions; Experiments; Factor analysis; Feature extraction; Flow theory; Fuzzy logic; Identification; Studies; Traffic flow; Virtual reality}
}
@inproceedings{Wang_2021,
	title        = {{DCN} V2: Improved Deep \& Cross Network and Practical Lessons for Web-scale Learning to Rank Systems},
	author       = {Ruoxi Wang and Rakesh Shivanna and Derek Cheng and Sagar Jain and Dong Lin and Lichan Hong and Ed Chi},
	year         = 2021,
	month        = {apr},
	booktitle    = {Proceedings of the Web Conference 2021},
	pages        = {1785--1797}
}
@article{Wang_2022,
	title        = {A Review of Vehicle Detection Techniques for Intelligent Vehicles},
	author       = {Zhangu Wang and Jun Zhan and Chunguang Duan and Xin Guan and Pingping Lu and Kai Yang},
	year         = 2022,
	journal      = {IEEE Transactions on Neural Networks and Learning Systems},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	pages        = {1--21},
	abstract     = {Robust and efficient vehicle detection is an important task of environment perception of intelligent vehicles, which directly affects the behavior decision-making and motion planning of intelligent vehicles. Due to the rapid development of sensor and computer technology, the algorithm and technology of vehicle detection have been updated rapidly. But, there are few reviews on vehicle detection of intelligent vehicles, especially covering all kinds of sensors and algorithms in recent years. This article presents a comprehensive review of vehicle detection approaches and their applications in intelligent vehicle systems to analyze the development of vehicle detection, with a specific focus on sensor types and algorithm classification. First, more than 300 research contributions are summarized in this review, including all kinds of vehicle detection sensors (machine vision, millimeter-wave radar, lidar, and multisensor fusion), and the performance of the classic and latest algorithms was compared in detail. Then, the application scenarios of vehicle detection with different sensors and algorithms were analyzed according to their performance and applicability. Moreover, we also systematically summarized the methods of vehicle detection in adverse weather. Finally, the remaining challenges and future research trends were analyzed according to the development of intelligent vehicle sensors and algorithms. Robust and efficient vehicle detection is an important task of environment perception of intelligent vehicles, which directly affects the behavior decision-making and motion planning of intelligent vehicles. Due to the rapid development of sensor and computer technology, the algorithm and technology of vehicle detection have been updated rapidly. But, there are few reviews on vehicle detection of intelligent vehicles, especially covering all kinds of sensors and algorithms in recent years. This article presents a comprehensive review of vehicle detection approaches and their applications in intelligent vehicle systems to analyze the development of vehicle detection, with a specific focus on sensor types and algorithm classification. First, more than 300 research contributions are summarized in this review, including all kinds of vehicle detection sensors (machine vision, millimeter-wave radar, lidar, and multisensor fusion), and the performance of the classic and latest algorithms was compared in detail. Then, the application scenarios of vehicle detection with different sensors and algorithms were analyzed according to their performance and applicability. Moreover, we also systematically summarized the methods of vehicle detection in adverse weather. Finally, the remaining challenges and future research trends were analyzed according to the development of intelligent vehicle sensors and algorithms.},
	keywords     = {Vehicle detection; Feature extraction; Intelligent vehicles; Deep learning; Semantics; Machine vision; Image color analysis; Deep learning; information fusion; intelligent vehicle; sensors; vehicle detection}
}
@article{Wang_2023,
	title        = {Performance and Challenges of 3D Object Detection Methods in Complex Scenes for Autonomous Driving},
	author       = {Ke Wang and Tianqiang Zhou and Xingcan Li and Fan Ren},
	year         = 2023,
	month        = {feb},
	journal      = {{IEEE} Transactions on Intelligent Vehicles},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 8,
	number       = 2,
	pages        = {1699--1716}
}
@article{WangBochkovskiy-212,
	title        = {YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for  real-time object detectors},
	author       = {Chien-Yao Wang and Alexey Bochkovskiy and Hong-Yuan Mark Liao},
	year         = 2022,
	journal      = {arXiv.org},
	shortjournal = {arXiv.org}
}
@article{Wu_2018,
	title        = {{MsRi}-{CCF}: Multi-Scale and Rotation-Insensitive Convolutional Channel Features for Geospatial Object Detection},
	author       = {Xin Wu and Danfeng Hong and Pedram Ghamisi and Wei Li and Ran Tao},
	year         = 2018,
	month        = {dec},
	journal      = {Remote Sensing},
	publisher    = {{MDPI} {AG}},
	volume       = 10,
	number       = 12,
	pages        = 1990,
	issn         = {2072-4292},
	abstract     = {Geospatial object detection is a fundamental but challenging problem in the remote sensing community. Although deep learning has shown its power in extracting discriminative features, there is still room for improvement in its detection performance, particularly for objects with large ranges of variations in scale and direction. To this end, a novel approach, entitled multi-scale and rotation-insensitive convolutional channel features (MsRi-CCF), is proposed for geospatial object detection by integrating robust low-level feature generation, classifier generation with outlier removal, and detection with a power law. The low-level feature generation step consists of rotation-insensitive and multi-scale convolutional channel features, which were obtained by learning a regularized convolutional neural network (CNN) and integrating multi-scaled convolutional feature maps, followed by the fine-tuning of high-level connections in the CNN, respectively. Then, these generated features were fed into AdaBoost (chosen due to its lower computation and storage costs) with outlier removal to construct an object detection framework that facilitates robust classifier training. In the test phase, we adopted a log-space sampling approach instead of fine-scale sampling by using the fast feature pyramid strategy based on a computable power law. Extensive experimental results demonstrate that compared with several state-of-the-art baselines, the proposed MsRi-CCF approach yields better detection results, with 90.19\% precision with the satellite dataset and 81.44\% average precision with the NWPU VHR-10 datasets. Importantly, MsRi-CCF incurs no additional computational cost, which is only 0.92 s and 0.7 s per test image on the two datasets. Furthermore, we determined that most previous methods fail to gain an acceptable detection performance, particularly when they face several obstacles, such as deformations in objects (e.g., rotation, illumination, and scaling). Yet, these factors are effectively addressed by MsRi-CCF, yielding a robust geospatial object detection method.},
	language     = {en},
	shortjournal = {Remote Sensing}
}
@article{Xueyun_Chen_2014,
	title        = {Vehicle Detection in Satellite Images by Hybrid Deep Convolutional Neural Networks},
	author       = {Xueyun Chen and Shiming Xiang and Cheng-Lin Liu and Chun-Hong Pan},
	year         = 2014,
	journal      = {{IEEE} Geoscience and Remote Sensing Letters},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 11,
	number       = 10,
	pages        = {1797--1801},
	abstract     = {Detecting small objects such as vehicles in satellite images is a difficult problem. Many features (such as histogram of oriented gradient, local binary pattern, scale-invariant feature transform, etc.) have been used to improve the performance of object detection, but mostly in simple environments such as those on roads. Kembhavi et al. proposed that no satisfactory accuracy has been achieved in complex environments such as the City of San Francisco. Deep convolutional neural networks (DNNs) can learn rich features from the training data automatically and has achieved state-of-the-art performance in many image classification databases. Though the DNN has shown robustness to distortion, it only extracts features of the same scale, and hence is insufficient to tolerate large-scale variance of object. In this letter, we present a hybrid DNN (HDNN), by dividing the maps of the last convolutional layer and the max-pooling layer of DNN into multiple blocks of variable receptive field sizes or max-pooling field sizes, to enable the HDNN to extract variable-scale features. Comparative experimental results indicate that our proposed HDNN significantly outperforms the traditional DNN on vehicle detection.;~ Detecting small objects such as vehicles in satellite images is a difficult problem. Many features (such as histogram of oriented gradient, local binary pattern, scale-invariant featuretransform, etc.) have been used to improve the performance of object detection, but mostly in simple environments such as those on roads. Kembhavi et al. proposed that no satisfactory accuracy has been achieved in complex environments such as the City of San Francisco. Deep convolutional neural networks (DNNs) can learn rich features from the training data automatically and has achieved state-of-the-art performance in many image classification databases. Though the DNN has shown robustness to distortion, it only extracts features of the same scale, and hence is insufficient to tolerate large-scale variance of object. In this letter, we present a hybrid DNN (HDNN), by dividing the maps of the last convolutional layer and the max-pooling layer of DNN into multiple blocks of variable receptive field sizes or max-pooling field sizes, to enable the HDNN to extract variable-scale features. Comparative experimental results indicate that our proposed HDNN significantly outperforms the traditional DNN on vehicle detection. [PUBLICATION ABSTRACT];},
	keywords     = {Deep convolutional neural networks (DNNs); Feature extraction; hybrid DNNs (HDNNs); Neural networks; Object detection; Remote sensing; Satellites; Training; Vehicle detection; Vehicles}
}
@article{Yang_2018,
	title        = {Vehicle detection in intelligent transportation systems and its applications under varying environments: A review},
	author       = {Zi Yang and Lilian S.C. Pun-Cheng},
	year         = 2018,
	journal      = {Image and Vision Computing},
	publisher    = {Elsevier {BV}},
	volume       = 69,
	pages        = {143--154},
	abstract     = {Robust and efficient vehicle detection in monocular vision is an important task in Intelligent Transportation Systems. With the development of computer vision techniques and consequent accessibility of video image data, new applications have been enabled to on-road vehicle detection algorithms. This paper provides a review of the literature in vehicle detection under varying environments. Due to the variability of on-road driving environments, vehicle detection may face different problems and challenges. Therefore, many approaches have been proposed, and can be categorized as appearance-based methods and motion-based methods. In addition, special illumination, weather and driving scenarios are discussed in terms of methodology and quantitative evaluation. In the future, efforts should be focused on robust vehicle detection approaches for various on-road conditions.},
	keywords     = {Vehicle detection;Computer vision;Intelligent Transportation Systems;Varying environments;Traffic surveillance}
}
@inproceedings{Yang2021,
	title        = {SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks},
	author       = {Yang, Lingxiao and Zhang, Ru-Yuan and Li, Lida and Xie, Xiaohua},
	year         = 2021,
	month        = {18--24 Jul},
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	volume       = 139,
	pages        = {11863--11874},
	editor       = {Meila, Marina and Zhang, Tong},
}
@article{Yurtsever_2020,
	title        = {A Survey of Autonomous Driving: Common Practices and Emerging Technologies},
	author       = {Ekim Yurtsever and Jacob Lambert and Alexander Carballo and Kazuya Takeda},
	year         = 2020,
	journal      = {{IEEE} Access},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 8,
	pages        = {58443--58469},
	abstract     = {Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.;Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions: localization, mapping, perception, planning, and human machine interface, were thoroughly reviewed. Furthermore, the state-of-the-art was implemented on our own platform and various algorithms were compared in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.;IEEE Access, volume 8. 2020 Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions: localization, mapping, perception, planning, and human machine interface, were thoroughly reviewed. Furthermore, the state-of-the-art was implemented on our own platform and various algorithms were compared in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.;},
	keywords     = {Algorithms; Automation; Autonomous vehicles; control; intelligent transportation systems; intelligent vehicles; Man-machine interfaces; Mapping; New technology; robotics; State-of-the-art reviews}
}
@article{Zehang_Sun_2006,
	title        = {Monocular precrash vehicle detection: features and classifiers},
	author       = {Zehang Sun and G. Bebis and R. Miller},
	year         = 2006,
	month        = {jul},
	journal      = {IEEE Transactions on Image Processing},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 15,
	number       = 7,
	pages        = {2019--2034},
	abstract     = {Robust and reliable vehicle detection from images acquired by a moving vehicle (i.e., on-road vehicle detection) is an important problem with applications to driver assistance systems and autonomous, self-guided vehicles. The focus of this work is on the issues of feature extraction and classification for rear-view vehicle detection. Specifically, by treating the problem of vehicle detection as a two-class classification problem, we have investigated several different feature extraction methods such as principal component analysis, wavelets, and Gabor filters. To evaluate the extracted features, we have experimented with two popular classifiers, neural networks and support vector machines (SVMs). Based on our evaluation results, we have developed an on-board real-time monocular vehicle detection system that is capable of acquiring grey-scale images, using Ford's proprietary low-light camera, achieving an average detection rate of 10 Hz. Our vehicle detection algorithm consists of two main steps: a multiscale driven hypothesis generation step and an appearance-based hypothesis verification step. During the hypothesis generation step, image locations where vehicles might be present are extracted. This step uses multiscale techniques not only to speed up detection, but also to improve system robustness. The appearance-based hypothesis verification step verifies the hypotheses using Gabor features and SVMs. The system has been tested in Ford's concept vehicle under different traffic conditions (e.g., structured highway, complex urban streets, and varying weather conditions), illustrating good performance. Robust and reliable vehicle detection from images acquired by a moving vehicle (i.e., on-road vehicle detection) is an important problem with applications to driver assistance systems and autonomous, self-guided vehicles. The focus of this work is on the issues of feature extraction and classification for rear-view vehicle detection. Specifically, by treating the problem of vehicle detection as a two-class classification problem, we have investigated several different feature extraction methods such as principal component analysis, wavelets, and Gabor filters. To evaluate the extracted features, we have experimented with two popular classifiers, neural networks and support vector machines (SVMs). Based on our evaluation results, we have developed an on-board real-time monocular vehicle detection system that is capable of acquiring grey-scale images, using Ford's proprietary low-light camera, achieving an average detection rate of 10 Hz. Our vehicle detection algorithm consists of two main steps: a multiscale driven hypothesis generation step and an appearance-based hypothesis verification step. During the hypothesis generation step, image locations where vehicles might be present are extracted. This step uses multiscale techniques not only to speed up detection, but also to improve system robustness. The appearance-based hypothesis verification step verifies the hypotheses using Gabor features and SVMs. The system has been tested in Ford's concept vehicle under different traffic conditions (e.g., structured highway, complex urban streets, and varying weather conditions), illustrating good performance.},
	keywords     = {Vehicle detection; Computer vision; Feature extraction; Robustness; Remotely operated vehicles; Vehicle driving; Mobile robots; Focusing; Principal component analysis; Wavelet analysis; Gabor filters; neural networks (NNs); principal component analysis (PCA); support vector machines (SVMs); vehicle detection; wavelets}
}
@article{Zhang_2022,
	title        = {Real-Time Vehicle Detection Based on Improved {YOLO} v5},
	author       = {Yu Zhang and Zhongyin Guo and Jianqing Wu and Yuan Tian and Haotian Tang and Xinming Guo},
	year         = 2022,
	journal      = {Sustainability},
	publisher    = {{MDPI} {AG}},
	volume       = 14,
	number       = 19,
	pages        = 12274,
	abstract     = {To reduce the false detection rate of vehicle targets caused by occlusion, an improved method of vehicle detection in different traffic scenarios based on an improved YOLO v5 network is proposed. The proposed method uses the Flip-Mosaic algorithm to enhance the network's perception of small targets. A multi-type vehicle target dataset collected in different scenarios was set up. The detection model was trained based on the dataset. The experimental results showed that the Flip-Mosaic data enhancement algorithm can improve the accuracy of vehicle detection and reduce the false detection rate.},
	keywords     = {Accuracy; Algorithms; Boxes; Classification; Datasets; Efficiency; Flip-Mosaic algorithm; image processing; Mosaics; object detection; Occlusion; Target detection; Tolls; Traffic accidents \& safety; Traffic congestion; Traffic safety; YOLO v5}
}
@inproceedings{Zhao_2023,
	title        = {YOLOv7-sea: Object Detection of Maritime UAV Images based on Improved YOLOv7},
	author       = {Hangyue Zhao and Hongpu Zhang and Yanyun Zhao},
	year         = 2023,
	month        = {jan},
	booktitle    = {2023 {IEEE}/{CVF} Winter Conference on Applications of Computer Vision Workshops},
	pages        = {233--238},
	abstract     = {Object detection algorithms play an important role in maritime search and rescue missions, where they are designed to detect people, boats and other objects in open water. However, the SeaDronesee dataset has the characteristics of small targets and large sea surface interference, which brings great challenges to general object detectors. To address these issues, we propose an improved detector YOLOv7-sea. Based on YOLOv7[2], we add a prediction head to detect tiny-scale people or objects. Besides, we integrate Simple, Parameter-Free Attention Module (SimAM) to find attention regions in the scene. To achieve further improvements to our proposed YOLOv7-sea, we provide some useful strategies such as data augmentation, Test time augmentation (TTA), and bundled box fusion (WBF). On the ODv2 challenge dataset, the AP result of YOLOv7-sea is 59.00\%, which is about 7\% higher than the baseline model (YOLOv7). Object detection algorithms play an important role in maritime search and rescue missions, where they are designed to detect people, boats and other objects in open water. However, the SeaDronesee dataset has the characteristics of small targets and large sea surface interference, which brings great challenges to general object detectors. To address these issues, we propose an improved detector YOLOv7-sea. Based on YOLOv7[2], we add a prediction head to detect tiny-scale people or objects. Besides, we integrate Simple, Parameter-Free Attention Module (SimAM) to find attention regions in the scene. To achieve further improvements to our proposed YOLOv7-sea, we provide some useful strategies such as data augmentation, Test time augmentation (TTA), and bundled box fusion (WBF). On the ODv2 challenge dataset, the AP result of YOLOv7-sea is 59.00\%, which is about 7\% higher than the baseline model (YOLOv7).},
	keywords     = {Sea surface; Visualization; Head; Conferences; Detectors; Object detection; Interference}
}
@article{Zhu_2020,
	title        = {{MME}-{YOLO}: Multi-Sensor Multi-Level Enhanced {YOLO} for Robust Vehicle Detection in Traffic Surveillance},
	author       = {Jianxiao Zhu and Xu Li and Peng Jin and Qimin Xu and Zhengliang Sun and Xiang Song},
	year         = 2020,
	month        = {dec},
	journal      = {Sensors},
	publisher    = {{MDPI} {AG}},
	volume       = 21,
	number       = 1,
	pages        = 27,
	abstract     = {As an effective means of solving collision problems caused by the limited       perspective on board, the cooperative roadside system is gaining       popularity. To improve the vehicle detection abilities in such online       safety systems, in this paper, we propose a novel multi-sensor multi-level       enhanced convolutional network model, called multi-sensor multi-level       enhanced convolutional network architecture (MME-YOLO), with consideration       of hybrid realistic scene of scales, illumination, and occlusion. MME-YOLO       consists of two tightly coupled structures, i.e., the enhanced inference       head and the LiDAR-Image composite module. More specifically, the enhanced       inference head preliminarily equips the network with stronger inference       abilities for redundant visual cues by attention-guided feature selection       blocks and anchor-based/anchor-free ensemble head. Furthermore, the       LiDAR-Image composite module cascades the multi-level feature maps from       the LiDAR subnet to the image subnet, which strengthens the generalization       of the detector in complex scenarios. Compared with YOLOv3, the enhanced       inference head achieves a 5.83}
}
@inproceedings{Zhu_2021,
	title        = {TPH-YOLOv5: Improved YOLOv5 Based on Transformer Prediction Head for Object Detection on Drone-Captured Scenarios},
	author       = {Xingkui Zhu and Shuchang Lyu and Xu Wang and Qi Zhao},
	year         = 2021,
	month        = {oct},
	booktitle    = {2021 {IEEE}/{CVF} International Conference on Computer Vision Workshops},
	pages        = {2778--2788}
}
@article{li2022cross-domain,
	title        = {Cross-domain object detection for autonomous driving: A stepwise domain adaptative YOLO approach},
	author       = {Li, Guofa and Ji, Zefeng and Qu, Xingda and Zhou, Rui and Cao, Dongpu},
	year         = 2022,
	journal      = {IEEE Transactions on Intelligent Vehicles},
	publisher    = {IEEE},
	volume       = 7,
	number       = 3,
	pages        = {603--615}
}
@article{yan2024enhanced,
	title        = {Enhanced object detection in pediatric bronchoscopy images using YOLO-based algorithms with CBAM attention mechanism},
	author       = {Yan, Jianqi and Zeng, Yifan and Lin, Junhong and Pei, Zhiyuan and Fan, Jinrui and Fang, Chuanyu and Cai, Yong},
	year         = 2024,
	journal      = {Heliyon},
	publisher    = {Elsevier},
	volume       = 10,
	number       = 12
}
@article{cheng2024enhancing,
	title        = {Enhancing Remote Sensing Object Detection with K-CBST YOLO: Integrating CBAM and Swin-Transformer},
	author       = {Cheng, Aonan and Xiao, Jincheng and Li, Yingcheng and Sun, Yiming and Ren, Yafeng and Liu, Jianli},
	year         = 2024,
	journal      = {Remote Sensing},
	publisher    = {MDPI},
	volume       = 16,
	number       = 16,
	pages        = 2885
}
@article{shen2023ca-yolo,
	title        = {CA-YOLO: Model optimization for remote sensing image object detection},
	author       = {Shen, Lingyun and Lang, Baihe and Song, Zhengxun},
	year         = 2023,
	journal      = {Ieee Access},
	publisher    = {IEEE}
}
@article{jia2023mobilenet-ca-yolo,
	title        = {MobileNet-CA-YOLO: An improved YOLOv7 based on the MobileNetV3 and attention mechanism for Rice pests and diseases detection},
	author       = {Jia, Liangquan and Wang, Tao and Chen, Yi and Zang, Ying and Li, Xiangge and Shi, Haojie and Gao, Lu},
	year         = 2023,
	journal      = {Agriculture},
	publisher    = {MDPI},
	volume       = 13,
	number       = 7,
	pages        = 1285
}
@article{wu2023yolo-se,
	title        = {YOLO-SE: Improved YOLOv8 for remote sensing object detection and recognition},
	author       = {Wu, Tianyong and Dong, Youkou},
	year         = 2023,
	journal      = {Applied Sciences},
	publisher    = {MDPI},
	volume       = 13,
	number       = 24,
	pages        = 12977
}
@article{mahaadevan2023automatic,
	title        = {Automatic identifier of socket for electrical vehicles using SWIN-transformer and SimAM attention mechanism-based EVS YOLO},
	author       = {Mahaadevan, VC and Narayanamoorthi, R and Gono, Radomir and Moldrik, Petr},
	year         = 2023,
	journal      = {IEEE Access},
	publisher    = {IEEE}
}
